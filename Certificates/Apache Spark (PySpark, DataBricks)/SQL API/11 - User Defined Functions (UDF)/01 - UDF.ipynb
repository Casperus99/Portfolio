{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a59aa28-8baa-4ca5-928b-c0dfe13308e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orders = spark.read.json('/public/retail_db_json/orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4e0866f-de60-4963-b9ea-2111396518b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# UDF (User Defined Function)\n",
    "\n",
    "# Registering new function\n",
    "dc = spark.udf.register('date_convert', lambda d: int(d[:10].replace('-', '')))\n",
    "\n",
    "# Usage in basic syntax\n",
    "orders.select(dc('order_date').alias('order_date')).show(n=5)\n",
    "\n",
    "# Usage in SQL and selectExpr\n",
    "spark.sql(\"\"\"\n",
    "    SELECT o.*, date_convert(order_date) AS order_date_as_int\n",
    "    FROM orders AS o\n",
    "\"\"\").show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ad7585c-0ae5-4b4d-b9d2-77fb60a2aed1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We are able to define new spark-friendly functions with .register(name, func):\n",
    "\n",
    "dc = spark.udf.register('date_convert', lambda d: int(d[:10].replace('-', '')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ded06a67-6fa0-4be5-94be-c796d13c1f5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------------+--------+---------------+\n|order_customer_id|order_date           |order_id|order_status   |\n+-----------------+---------------------+--------+---------------+\n|11599            |2013-07-25 00:00:00.0|1       |CLOSED         |\n|256              |2013-07-25 00:00:00.0|2       |PENDING_PAYMENT|\n|12111            |2013-07-25 00:00:00.0|3       |COMPLETE       |\n|8827             |2013-07-25 00:00:00.0|4       |CLOSED         |\n|11318            |2013-07-25 00:00:00.0|5       |COMPLETE       |\n+-----------------+---------------------+--------+---------------+\nonly showing top 5 rows\n\n+----------+\n|order_date|\n+----------+\n|  20130725|\n|  20130725|\n|  20130725|\n|  20130725|\n|  20130725|\n+----------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# In basic spark syntax we would use new variable name\n",
    "\n",
    "orders.show(n=5, truncate=False)\n",
    "orders.select(dc('order_date').alias('order_date')).show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74500588-b54c-43fc-a768-ed4e2cb84996",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------------+--------+---------------+-----------------+\n|order_customer_id|order_date           |order_id|order_status   |order_date_as_int|\n+-----------------+---------------------+--------+---------------+-----------------+\n|11599            |2013-07-25 00:00:00.0|1       |CLOSED         |20130725         |\n|256              |2013-07-25 00:00:00.0|2       |PENDING_PAYMENT|20130725         |\n|12111            |2013-07-25 00:00:00.0|3       |COMPLETE       |20130725         |\n|8827             |2013-07-25 00:00:00.0|4       |CLOSED         |20130725         |\n|11318            |2013-07-25 00:00:00.0|5       |COMPLETE       |20130725         |\n+-----------------+---------------------+--------+---------------+-----------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# But in SQL expressions we would use the name:\n",
    "\n",
    "orders.createOrReplaceTempView('orders')\n",
    "spark.sql(\"\"\"\n",
    "    SELECT o.*, date_convert(order_date) AS order_date_as_int\n",
    "    FROM orders AS o\n",
    "\"\"\").show(n=5, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01 - UDF",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}