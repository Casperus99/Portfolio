{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc4fc765-8a0c-4894-9bda-8ec73a4e3918",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "order_items = spark.read.json('/public/retail_db_json/order_items')\n",
    "orders = spark.read.json('/public/retail_db_json/orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d33aea6-844e-4a2c-969b-49c6e11e8362",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# AGGREGATIONS\n",
    "\n",
    "# Basic total aggregation\n",
    "orders.select(count('*').alias('count'), sum('order_id').alias('sum')).show()\n",
    "\n",
    "# Aggregation by specific column\n",
    "order_items.groupBy('order_item_id').sum('order_item_quantity').show(n=10)\n",
    "\n",
    "# Aggregation using .agg() method - far more flexible\n",
    "order_items\\\n",
    "    .groupBy('order_item_id')\\\n",
    "    .agg(sum('order_item_quantity'), sum('order_item_subtotal').alias('order_revenue'), min('order_item_quantity'))\\\n",
    "    .show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27a80213-f283-4a64-b3a3-5cadb1dad3fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n|count|       sum|\n+-----+----------+\n|68883|2372468286|\n+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Basic aggregation of whole dataset (Total Aggregation)\n",
    "\n",
    "orders.select(count('*').alias('count'), sum('order_id').alias('sum')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cf490e4-4b1c-4fb0-9934-868d47633a10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "68883"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .count() method can be used directly on a dataset\n",
    "\n",
    "orders.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1965b051-ce2a-471c-a63c-abce54331973",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "pyspark.sql.group.GroupedData"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .groupBy method creates GroupedData datatype.\n",
    "# It allows us to use aggregate by specific groups.\n",
    "\n",
    "type(order_items.groupBy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eac440ad-232c-47b6-a720-71d66702a321",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------------+--------------------------+-----------------------------+------------------------+------------------------+\n|min(order_item_id)|min(order_item_order_id)|min(order_item_product_id)|min(order_item_product_price)|min(order_item_quantity)|min(order_item_subtotal)|\n+------------------+------------------------+--------------------------+-----------------------------+------------------------+------------------------+\n|                 1|                       1|                        19|                         9.99|                       1|                    9.99|\n+------------------+------------------------+--------------------------+-----------------------------+------------------------+------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# .groupBy() without an argument lets us aggregate the whole dataset (that can be aggregated with used function)\n",
    "# For example: min() will not return the value for a string column\n",
    "\n",
    "order_items.groupBy().min().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3512db8d-9b96-4f6b-8961-79870840e7d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------------+\n|order_item_id|sum(order_item_quantity)|\n+-------------+------------------------+\n|           26|                       1|\n|           29|                       1|\n|          474|                       3|\n|          964|                       3|\n|         1677|                       2|\n|         1697|                       1|\n|         1806|                       2|\n|         1950|                       1|\n|         2040|                       5|\n|         2214|                       4|\n+-------------+------------------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# .groupBy() with an argument lets us aggregate the dataset by specific columns\n",
    "# .sum(*cols) will return the sum of all values for the given columns\n",
    "\n",
    "order_items.groupBy('order_item_id').sum('order_item_quantity').show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fb844ed-830b-4d18-bd6a-4ee222039a5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPySparkAttributeError\u001B[0m                     Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2567374494928662>, line 6\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# But this syntax is quite limited - we can use only one aggr function\u001B[39;00m\n",
       "\u001B[1;32m      3\u001B[0m order_items \\\n",
       "\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39mgroupBy(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124morder_item_id\u001B[39m\u001B[38;5;124m'\u001B[39m) \\\n",
       "\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39msum(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124morder_item_quantity\u001B[39m\u001B[38;5;124m'\u001B[39m) \\\n",
       "\u001B[0;32m----> 6\u001B[0m     \u001B[38;5;241m.\u001B[39mmin(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124morder_item_quantity\u001B[39m\u001B[38;5;124m'\u001B[39m) \\\n",
       "\u001B[1;32m      7\u001B[0m     \u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3753\u001B[0m, in \u001B[0;36mDataFrame.__getattr__\u001B[0;34m(self, name)\u001B[0m\n",
       "\u001B[1;32m   3720\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001B[39;00m\n",
       "\u001B[1;32m   3721\u001B[0m \n",
       "\u001B[1;32m   3722\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.3.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3750\u001B[0m \u001B[38;5;124;03m+---+\u001B[39;00m\n",
       "\u001B[1;32m   3751\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   3752\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns:\n",
       "\u001B[0;32m-> 3753\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkAttributeError(\n",
       "\u001B[1;32m   3754\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mATTRIBUTE_NOT_SUPPORTED\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattr_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: name}\n",
       "\u001B[1;32m   3755\u001B[0m     )\n",
       "\u001B[1;32m   3756\u001B[0m jc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jdf\u001B[38;5;241m.\u001B[39mapply(name)\n",
       "\u001B[1;32m   3757\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Column(jc)\n",
       "\n",
       "\u001B[0;31mPySparkAttributeError\u001B[0m: [ATTRIBUTE_NOT_SUPPORTED] Attribute `min` is not supported."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "PySparkAttributeError",
        "evalue": "[ATTRIBUTE_NOT_SUPPORTED] Attribute `min` is not supported."
       },
       "metadata": {
        "errorSummary": "[ATTRIBUTE_NOT_SUPPORTED] Attribute `min` is not supported."
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "ATTRIBUTE_NOT_SUPPORTED",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "sqlState": null,
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPySparkAttributeError\u001B[0m                     Traceback (most recent call last)",
        "File \u001B[0;32m<command-2567374494928662>, line 6\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# But this syntax is quite limited - we can use only one aggr function\u001B[39;00m\n\u001B[1;32m      3\u001B[0m order_items \\\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39mgroupBy(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124morder_item_id\u001B[39m\u001B[38;5;124m'\u001B[39m) \\\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39msum(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124morder_item_quantity\u001B[39m\u001B[38;5;124m'\u001B[39m) \\\n\u001B[0;32m----> 6\u001B[0m     \u001B[38;5;241m.\u001B[39mmin(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124morder_item_quantity\u001B[39m\u001B[38;5;124m'\u001B[39m) \\\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;241m.\u001B[39mshow()\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3753\u001B[0m, in \u001B[0;36mDataFrame.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   3720\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001B[39;00m\n\u001B[1;32m   3721\u001B[0m \n\u001B[1;32m   3722\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3750\u001B[0m \u001B[38;5;124;03m+---+\u001B[39;00m\n\u001B[1;32m   3751\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   3752\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns:\n\u001B[0;32m-> 3753\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkAttributeError(\n\u001B[1;32m   3754\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mATTRIBUTE_NOT_SUPPORTED\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattr_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: name}\n\u001B[1;32m   3755\u001B[0m     )\n\u001B[1;32m   3756\u001B[0m jc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jdf\u001B[38;5;241m.\u001B[39mapply(name)\n\u001B[1;32m   3757\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Column(jc)\n",
        "\u001B[0;31mPySparkAttributeError\u001B[0m: [ATTRIBUTE_NOT_SUPPORTED] Attribute `min` is not supported."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# But this syntax is quite limited - we can use only one aggr function\n",
    "\n",
    "order_items \\\n",
    "    .groupBy('order_item_id') \\\n",
    "    .sum('order_item_quantity') \\\n",
    "    .min('order_item_quantity') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4da7a13-d5c7-411c-b005-7e97d195a00b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+-------------+\n|order_item_order_id|order_quantity|order_revenue|\n+-------------------+--------------+-------------+\n|                 26|             1|       129.99|\n|                 29|             1|        59.99|\n|                474|             3|       119.97|\n|                964|             3|       119.97|\n|               1677|             2|        99.96|\n|               1697|             1|       399.98|\n|               1806|             2|       119.98|\n|               1950|             1|        49.98|\n|               2040|             5|       499.95|\n|               2214|             4|       239.96|\n+-------------------+--------------+-------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# And editing aggr column is also difficult\n",
    "\n",
    "order_items\\\n",
    "    .groupBy('order_item_id')\\\n",
    "    .sum('order_item_quantity', 'order_item_subtotal')\\\n",
    "    .toDF('order_item_order_id', 'order_quantity', 'order_revenue')\\\n",
    "    .withColumn('order_revenue', round('order_revenue', 2))\\\n",
    "    .show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0adb009d-8d2a-40e5-81cb-2be44180926e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------------+-------------+------------------------+\n|order_item_id|sum(order_item_quantity)|order_revenue|min(order_item_quantity)|\n+-------------+------------------------+-------------+------------------------+\n|           26|                       1|       129.99|                       1|\n|           29|                       1|        59.99|                       1|\n|          474|                       3|       119.97|                       3|\n|          964|                       3|       119.97|                       3|\n|         1677|                       2|        99.96|                       2|\n+-------------+------------------------+-------------+------------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# A better way is to use .agg() method\n",
    "# Then we can use multiple aggr functions and edit the aggr columns on the fly\n",
    "\n",
    "order_items\\\n",
    "    .groupBy('order_item_id')\\\n",
    "    .agg(sum('order_item_quantity'), sum('order_item_subtotal').alias('order_revenue'), min('order_item_quantity'))\\\n",
    "    .show(n=5)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01 - Aggregations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}