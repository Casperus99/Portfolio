{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2035ff5e-737b-4a45-8f21-22bf751052ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SINGLE COLUMN DATAFRAME FROM LIST\n",
    "\n",
    "ages = [1,2,3]\n",
    "spark.createDataFrame(ages, schema='int')  # You've got to use schema...\n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "spark.createDataFrame(ages, schema=IntegerType())\n",
    "\n",
    "ages = [(13, ), (23, ), (45, ), (90, )]  # ...unless\n",
    "spark.createDataFrame(ages)\n",
    "spark.createDataFrame(ages, 'col_name int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee507d44-ac38-4a6f-8305-6d92fffde259",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MULTI COLUMN DATAFRAME FROM LIST\n",
    "\n",
    "list_of_rows = [(1, \"Kacper\"), (2, \"Michael\"), (3, \"Sophia\")]\n",
    "spark.createDataFrame(list_of_rows)\n",
    "spark.createDataFrame(list_of_rows, 'id int, name string')\n",
    "\n",
    "column_names = ['id', 'name']\n",
    "spark.createDataFrame(list_of_rows, column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bb032a0-e2d5-4116-904c-062767c32fe9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SPARK ROWS - OVERVIEW\n",
    "list_of_lists = [[1, \"Kacper\"], [2, \"Michael\"], [3, \"Sophia\"]]\n",
    "df = spark.createDataFrame(list_of_lists, 'id int, name string')\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df.collect()  # Returns the dataframe as a list\n",
    "\n",
    "Row(1, \"Kacper\")\n",
    "row = Row(id=1, name=\"Kacper\")\n",
    "row.name\n",
    "row[\"name\"]\n",
    "\n",
    "list_of_rows = [Row(*list_obj) for list_obj in list_of_lists]\n",
    "spark.createDataFrame(list_of_rows, 'id int, name string').show()\n",
    "# It is possible to convert all those containers into Row objects but spark doesn't really need it to create a DataFrame correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83b708f5-6345-4133-84b9-e8dd816d5aaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# BASIC DATA TYPES\n",
    "\n",
    "\"\"\"\n",
    "Python      ->PySpark DataFrame\n",
    "\n",
    "int         ->bigint (long)\n",
    "string      ->string\n",
    "boolean     ->boolean\n",
    "float       ->double\n",
    "date        ->date\n",
    "datetime    ->timestamp\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d2e9f88-a30f-4ed7-a7f7-0a1d910d64e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SPECIFYING SCHEMA\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "df_schema_str = \"id INT, first_name STRING\"  # STRING\n",
    "df_schema_list = [\"id\" , \"first_name\"]  # LIST\n",
    "df_schema_spark_types = StructType([  # SPARK TYPES\n",
    "    StructField('id', IntegerType()),\n",
    "    StructField('first_name', StringType())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4252391-899d-4b78-a848-e4fb4227e471",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SPARK DATAFRAME FROM PANDAS DATAFRAME\n",
    "\n",
    "# 1. You can create a Spark DataFrame from a Pandas DataFrame:\n",
    "spark_df = spark.createDataFrame(pd.DataFrame(users))\n",
    "# 2. The biggest gain from using Pandas DataFrames is that it handles missing values:\n",
    "spark_df = spark.createDataFrame([Row(**user) for user in users])  # Error\n",
    "spark_df = spark.createDataFrame(pd.DataFrame(users))  # Works fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f88e27f-ee8c-4962-b0d7-e81ed0dc5452",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3 container type columns:\n",
    "# Arrays <-- list (index notation)\n",
    "# Maps <-- dict (keyword notation)\n",
    "# Structs <-- pySpark's Row (dot and keyword notation)\n",
    "\n",
    "# col(container_type_column) - lets us access the elements of the container column\n",
    "# col(XXX).alias('new_name') - SELECT XXX AS new_name\n",
    "# explode(container_type_column) - lets us explode the rows with these columns into multiple rows"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "08 - Summary",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
