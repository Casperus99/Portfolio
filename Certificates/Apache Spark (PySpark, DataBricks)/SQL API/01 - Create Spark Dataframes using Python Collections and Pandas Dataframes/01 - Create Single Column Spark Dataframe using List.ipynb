{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e947945d-6edf-49ef-b020-43130af9928c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SINGLE COLUMN DATAFRAME FROM LIST\n",
    "\n",
    "ages = [1,2,3]\n",
    "spark.createDataFrame(ages, schema='int')  # You've got to use schema...\n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "spark.createDataFrame(ages, schema=IntegerType())\n",
    "\n",
    "ages = [(13, ), (23, ), (45, ), (90, )]  # ...unless\n",
    "spark.createDataFrame(ages)\n",
    "spark.createDataFrame(ages, 'col_name int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "692f2dee-dbc6-4d5a-b77d-793f4742c1e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ages = [13, 23, 45, 90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e0c64e0-211a-491a-81fc-4d57bb3ae0fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPySparkTypeError\u001B[0m                          Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-8490600654114443>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# We can't create a new DataFrame just like that\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m dataframe \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(ages)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1605\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n",
       "\u001B[1;32m   1600\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n",
       "\u001B[1;32m   1601\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n",
       "\u001B[1;32m   1602\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateDataFrame(  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n",
       "\u001B[1;32m   1603\u001B[0m         data, schema, samplingRatio, verifySchema\n",
       "\u001B[1;32m   1604\u001B[0m     )\n",
       "\u001B[0;32m-> 1605\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_dataframe(\n",
       "\u001B[1;32m   1606\u001B[0m     data, schema, samplingRatio, verifySchema  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n",
       "\u001B[1;32m   1607\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1662\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n",
       "\u001B[1;32m   1660\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromRDD(data\u001B[38;5;241m.\u001B[39mmap(prepare), schema, samplingRatio)\n",
       "\u001B[1;32m   1661\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1662\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromLocal(\u001B[38;5;28mmap\u001B[39m(prepare, data), schema)\n",
       "\u001B[1;32m   1663\u001B[0m jrdd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mSerDeUtil\u001B[38;5;241m.\u001B[39mtoJavaArray(rdd\u001B[38;5;241m.\u001B[39m_to_java_object_rdd())\n",
       "\u001B[1;32m   1664\u001B[0m jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsparkSession\u001B[38;5;241m.\u001B[39mapplySchemaToPythonRDD(jrdd\u001B[38;5;241m.\u001B[39mrdd(), struct\u001B[38;5;241m.\u001B[39mjson())\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1229\u001B[0m, in \u001B[0;36mSparkSession._createFromLocal\u001B[0;34m(self, data, schema)\u001B[0m\n",
       "\u001B[1;32m   1221\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_createFromLocal\u001B[39m(\n",
       "\u001B[1;32m   1222\u001B[0m     \u001B[38;5;28mself\u001B[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001B[38;5;28mstr\u001B[39m]]]\n",
       "\u001B[1;32m   1223\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRDD[Tuple]\u001B[39m\u001B[38;5;124m\"\u001B[39m, StructType]:\n",
       "\u001B[1;32m   1224\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   1225\u001B[0m \u001B[38;5;124;03m    Create an RDD for DataFrame from a list or pandas.DataFrame, returns the RDD and schema.\u001B[39;00m\n",
       "\u001B[1;32m   1226\u001B[0m \u001B[38;5;124;03m    This would be broken with table acl enabled as user process does not have permission to\u001B[39;00m\n",
       "\u001B[1;32m   1227\u001B[0m \u001B[38;5;124;03m    write temp files.\u001B[39;00m\n",
       "\u001B[1;32m   1228\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 1229\u001B[0m     internal_data, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wrap_data_schema(data, schema)\n",
       "\u001B[1;32m   1230\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39mparallelize(internal_data), struct\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1196\u001B[0m, in \u001B[0;36mSparkSession._wrap_data_schema\u001B[0;34m(self, data, schema)\u001B[0m\n",
       "\u001B[1;32m   1193\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(data)\n",
       "\u001B[1;32m   1195\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n",
       "\u001B[0;32m-> 1196\u001B[0m     struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inferSchemaFromList(data, names\u001B[38;5;241m=\u001B[39mschema)\n",
       "\u001B[1;32m   1197\u001B[0m     converter \u001B[38;5;241m=\u001B[39m _create_converter(struct)\n",
       "\u001B[1;32m   1198\u001B[0m     tupled_data: Iterable[Tuple] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmap\u001B[39m(converter, data)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1061\u001B[0m, in \u001B[0;36mSparkSession._inferSchemaFromList\u001B[0;34m(self, data, names)\u001B[0m\n",
       "\u001B[1;32m   1059\u001B[0m infer_map_from_first_pair \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jconf\u001B[38;5;241m.\u001B[39mlegacyInferMapStructTypeFromFirstItem()\n",
       "\u001B[1;32m   1060\u001B[0m prefer_timestamp_ntz \u001B[38;5;241m=\u001B[39m is_timestamp_ntz_preferred()\n",
       "\u001B[0;32m-> 1061\u001B[0m schema \u001B[38;5;241m=\u001B[39m reduce(\n",
       "\u001B[1;32m   1062\u001B[0m     _merge_type,\n",
       "\u001B[1;32m   1063\u001B[0m     (\n",
       "\u001B[1;32m   1064\u001B[0m         _infer_schema(\n",
       "\u001B[1;32m   1065\u001B[0m             row,\n",
       "\u001B[1;32m   1066\u001B[0m             names,\n",
       "\u001B[1;32m   1067\u001B[0m             infer_dict_as_struct\u001B[38;5;241m=\u001B[39minfer_dict_as_struct,\n",
       "\u001B[1;32m   1068\u001B[0m             infer_array_from_first_element\u001B[38;5;241m=\u001B[39minfer_array_from_first_element,\n",
       "\u001B[1;32m   1069\u001B[0m             infer_map_from_first_pair\u001B[38;5;241m=\u001B[39minfer_map_from_first_pair,\n",
       "\u001B[1;32m   1070\u001B[0m             prefer_timestamp_ntz\u001B[38;5;241m=\u001B[39mprefer_timestamp_ntz,\n",
       "\u001B[1;32m   1071\u001B[0m         )\n",
       "\u001B[1;32m   1072\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m row \u001B[38;5;129;01min\u001B[39;00m data\n",
       "\u001B[1;32m   1073\u001B[0m     ),\n",
       "\u001B[1;32m   1074\u001B[0m )\n",
       "\u001B[1;32m   1075\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _has_nulltype(schema):\n",
       "\u001B[1;32m   1076\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkValueError(\n",
       "\u001B[1;32m   1077\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCANNOT_DETERMINE_TYPE\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   1078\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{},\n",
       "\u001B[1;32m   1079\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1064\u001B[0m, in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n",
       "\u001B[1;32m   1059\u001B[0m infer_map_from_first_pair \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jconf\u001B[38;5;241m.\u001B[39mlegacyInferMapStructTypeFromFirstItem()\n",
       "\u001B[1;32m   1060\u001B[0m prefer_timestamp_ntz \u001B[38;5;241m=\u001B[39m is_timestamp_ntz_preferred()\n",
       "\u001B[1;32m   1061\u001B[0m schema \u001B[38;5;241m=\u001B[39m reduce(\n",
       "\u001B[1;32m   1062\u001B[0m     _merge_type,\n",
       "\u001B[1;32m   1063\u001B[0m     (\n",
       "\u001B[0;32m-> 1064\u001B[0m         _infer_schema(\n",
       "\u001B[1;32m   1065\u001B[0m             row,\n",
       "\u001B[1;32m   1066\u001B[0m             names,\n",
       "\u001B[1;32m   1067\u001B[0m             infer_dict_as_struct\u001B[38;5;241m=\u001B[39minfer_dict_as_struct,\n",
       "\u001B[1;32m   1068\u001B[0m             infer_array_from_first_element\u001B[38;5;241m=\u001B[39minfer_array_from_first_element,\n",
       "\u001B[1;32m   1069\u001B[0m             infer_map_from_first_pair\u001B[38;5;241m=\u001B[39minfer_map_from_first_pair,\n",
       "\u001B[1;32m   1070\u001B[0m             prefer_timestamp_ntz\u001B[38;5;241m=\u001B[39mprefer_timestamp_ntz,\n",
       "\u001B[1;32m   1071\u001B[0m         )\n",
       "\u001B[1;32m   1072\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m row \u001B[38;5;129;01min\u001B[39;00m data\n",
       "\u001B[1;32m   1073\u001B[0m     ),\n",
       "\u001B[1;32m   1074\u001B[0m )\n",
       "\u001B[1;32m   1075\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _has_nulltype(schema):\n",
       "\u001B[1;32m   1076\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkValueError(\n",
       "\u001B[1;32m   1077\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCANNOT_DETERMINE_TYPE\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   1078\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{},\n",
       "\u001B[1;32m   1079\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2320\u001B[0m, in \u001B[0;36m_infer_schema\u001B[0;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, infer_map_from_first_pair, prefer_timestamp_ntz)\u001B[0m\n",
       "\u001B[1;32m   2317\u001B[0m     items \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msorted\u001B[39m(row\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m\u001B[38;5;241m.\u001B[39mitems())\n",
       "\u001B[1;32m   2319\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 2320\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n",
       "\u001B[1;32m   2321\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCANNOT_INFER_SCHEMA_FOR_TYPE\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   2322\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(row)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n",
       "\u001B[1;32m   2323\u001B[0m     )\n",
       "\u001B[1;32m   2325\u001B[0m fields \u001B[38;5;241m=\u001B[39m []\n",
       "\u001B[1;32m   2326\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m items:\n",
       "\n",
       "\u001B[0;31mPySparkTypeError\u001B[0m: [CANNOT_INFER_SCHEMA_FOR_TYPE] Can not infer schema for type: `int`."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "PySparkTypeError",
        "evalue": "[CANNOT_INFER_SCHEMA_FOR_TYPE] Can not infer schema for type: `int`."
       },
       "metadata": {
        "errorSummary": "[CANNOT_INFER_SCHEMA_FOR_TYPE] Can not infer schema for type: `int`."
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "CANNOT_INFER_SCHEMA_FOR_TYPE",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "sqlState": null,
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPySparkTypeError\u001B[0m                          Traceback (most recent call last)",
        "File \u001B[0;32m<command-8490600654114443>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# We can't create a new DataFrame just like that\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m dataframe \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(ages)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1605\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m   1600\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n\u001B[1;32m   1601\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n\u001B[1;32m   1602\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateDataFrame(  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n\u001B[1;32m   1603\u001B[0m         data, schema, samplingRatio, verifySchema\n\u001B[1;32m   1604\u001B[0m     )\n\u001B[0;32m-> 1605\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_dataframe(\n\u001B[1;32m   1606\u001B[0m     data, schema, samplingRatio, verifySchema  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m   1607\u001B[0m )\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1662\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m   1660\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromRDD(data\u001B[38;5;241m.\u001B[39mmap(prepare), schema, samplingRatio)\n\u001B[1;32m   1661\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1662\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromLocal(\u001B[38;5;28mmap\u001B[39m(prepare, data), schema)\n\u001B[1;32m   1663\u001B[0m jrdd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mSerDeUtil\u001B[38;5;241m.\u001B[39mtoJavaArray(rdd\u001B[38;5;241m.\u001B[39m_to_java_object_rdd())\n\u001B[1;32m   1664\u001B[0m jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsparkSession\u001B[38;5;241m.\u001B[39mapplySchemaToPythonRDD(jrdd\u001B[38;5;241m.\u001B[39mrdd(), struct\u001B[38;5;241m.\u001B[39mjson())\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1229\u001B[0m, in \u001B[0;36mSparkSession._createFromLocal\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m   1221\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_createFromLocal\u001B[39m(\n\u001B[1;32m   1222\u001B[0m     \u001B[38;5;28mself\u001B[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001B[38;5;28mstr\u001B[39m]]]\n\u001B[1;32m   1223\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRDD[Tuple]\u001B[39m\u001B[38;5;124m\"\u001B[39m, StructType]:\n\u001B[1;32m   1224\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1225\u001B[0m \u001B[38;5;124;03m    Create an RDD for DataFrame from a list or pandas.DataFrame, returns the RDD and schema.\u001B[39;00m\n\u001B[1;32m   1226\u001B[0m \u001B[38;5;124;03m    This would be broken with table acl enabled as user process does not have permission to\u001B[39;00m\n\u001B[1;32m   1227\u001B[0m \u001B[38;5;124;03m    write temp files.\u001B[39;00m\n\u001B[1;32m   1228\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1229\u001B[0m     internal_data, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wrap_data_schema(data, schema)\n\u001B[1;32m   1230\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39mparallelize(internal_data), struct\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1196\u001B[0m, in \u001B[0;36mSparkSession._wrap_data_schema\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m   1193\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(data)\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[0;32m-> 1196\u001B[0m     struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inferSchemaFromList(data, names\u001B[38;5;241m=\u001B[39mschema)\n\u001B[1;32m   1197\u001B[0m     converter \u001B[38;5;241m=\u001B[39m _create_converter(struct)\n\u001B[1;32m   1198\u001B[0m     tupled_data: Iterable[Tuple] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmap\u001B[39m(converter, data)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1061\u001B[0m, in \u001B[0;36mSparkSession._inferSchemaFromList\u001B[0;34m(self, data, names)\u001B[0m\n\u001B[1;32m   1059\u001B[0m infer_map_from_first_pair \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jconf\u001B[38;5;241m.\u001B[39mlegacyInferMapStructTypeFromFirstItem()\n\u001B[1;32m   1060\u001B[0m prefer_timestamp_ntz \u001B[38;5;241m=\u001B[39m is_timestamp_ntz_preferred()\n\u001B[0;32m-> 1061\u001B[0m schema \u001B[38;5;241m=\u001B[39m reduce(\n\u001B[1;32m   1062\u001B[0m     _merge_type,\n\u001B[1;32m   1063\u001B[0m     (\n\u001B[1;32m   1064\u001B[0m         _infer_schema(\n\u001B[1;32m   1065\u001B[0m             row,\n\u001B[1;32m   1066\u001B[0m             names,\n\u001B[1;32m   1067\u001B[0m             infer_dict_as_struct\u001B[38;5;241m=\u001B[39minfer_dict_as_struct,\n\u001B[1;32m   1068\u001B[0m             infer_array_from_first_element\u001B[38;5;241m=\u001B[39minfer_array_from_first_element,\n\u001B[1;32m   1069\u001B[0m             infer_map_from_first_pair\u001B[38;5;241m=\u001B[39minfer_map_from_first_pair,\n\u001B[1;32m   1070\u001B[0m             prefer_timestamp_ntz\u001B[38;5;241m=\u001B[39mprefer_timestamp_ntz,\n\u001B[1;32m   1071\u001B[0m         )\n\u001B[1;32m   1072\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m row \u001B[38;5;129;01min\u001B[39;00m data\n\u001B[1;32m   1073\u001B[0m     ),\n\u001B[1;32m   1074\u001B[0m )\n\u001B[1;32m   1075\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _has_nulltype(schema):\n\u001B[1;32m   1076\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkValueError(\n\u001B[1;32m   1077\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCANNOT_DETERMINE_TYPE\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1078\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{},\n\u001B[1;32m   1079\u001B[0m     )\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1064\u001B[0m, in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   1059\u001B[0m infer_map_from_first_pair \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jconf\u001B[38;5;241m.\u001B[39mlegacyInferMapStructTypeFromFirstItem()\n\u001B[1;32m   1060\u001B[0m prefer_timestamp_ntz \u001B[38;5;241m=\u001B[39m is_timestamp_ntz_preferred()\n\u001B[1;32m   1061\u001B[0m schema \u001B[38;5;241m=\u001B[39m reduce(\n\u001B[1;32m   1062\u001B[0m     _merge_type,\n\u001B[1;32m   1063\u001B[0m     (\n\u001B[0;32m-> 1064\u001B[0m         _infer_schema(\n\u001B[1;32m   1065\u001B[0m             row,\n\u001B[1;32m   1066\u001B[0m             names,\n\u001B[1;32m   1067\u001B[0m             infer_dict_as_struct\u001B[38;5;241m=\u001B[39minfer_dict_as_struct,\n\u001B[1;32m   1068\u001B[0m             infer_array_from_first_element\u001B[38;5;241m=\u001B[39minfer_array_from_first_element,\n\u001B[1;32m   1069\u001B[0m             infer_map_from_first_pair\u001B[38;5;241m=\u001B[39minfer_map_from_first_pair,\n\u001B[1;32m   1070\u001B[0m             prefer_timestamp_ntz\u001B[38;5;241m=\u001B[39mprefer_timestamp_ntz,\n\u001B[1;32m   1071\u001B[0m         )\n\u001B[1;32m   1072\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m row \u001B[38;5;129;01min\u001B[39;00m data\n\u001B[1;32m   1073\u001B[0m     ),\n\u001B[1;32m   1074\u001B[0m )\n\u001B[1;32m   1075\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _has_nulltype(schema):\n\u001B[1;32m   1076\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkValueError(\n\u001B[1;32m   1077\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCANNOT_DETERMINE_TYPE\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1078\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{},\n\u001B[1;32m   1079\u001B[0m     )\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2320\u001B[0m, in \u001B[0;36m_infer_schema\u001B[0;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, infer_map_from_first_pair, prefer_timestamp_ntz)\u001B[0m\n\u001B[1;32m   2317\u001B[0m     items \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msorted\u001B[39m(row\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m\u001B[38;5;241m.\u001B[39mitems())\n\u001B[1;32m   2319\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2320\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m   2321\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCANNOT_INFER_SCHEMA_FOR_TYPE\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   2322\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(row)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[1;32m   2323\u001B[0m     )\n\u001B[1;32m   2325\u001B[0m fields \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m   2326\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m items:\n",
        "\u001B[0;31mPySparkTypeError\u001B[0m: [CANNOT_INFER_SCHEMA_FOR_TYPE] Can not infer schema for type: `int`."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We can't create a new DataFrame just like that\n",
    "dataframe = spark.createDataFrame(ages) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5439dedd-99d2-43c5-877a-f76171982213",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n|value|\n+-----+\n|   13|\n|   23|\n|   45|\n|   90|\n+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# In case of a single column we need to specify the schema (data type of that column)\n",
    "spark.createDataFrame(ages, schema='int').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0a9be0e-9eb3-4ed4-9e4a-a82e9bf79011",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n|value|\n+-----+\n|   13|\n|   23|\n|   45|\n|   90|\n+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# We can also use data types from the pyspark library\n",
    "from pyspark.sql.types import IntegerType\n",
    "spark.createDataFrame(ages, schema=IntegerType()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b35b7643-a925-45b1-b896-8c5c4ae3d7c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n| _1|\n+---+\n| 13|\n| 23|\n| 45|\n| 90|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "# But there is one more way to do it...\n",
    "ages = [(13, ), (23, ), (45, ), (90, )]\n",
    "spark.createDataFrame(ages).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dfb9ef7-5955-4e5d-a486-9d89c60629a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n|ages|\n+----+\n|  13|\n|  23|\n|  45|\n|  90|\n+----+\n\n"
     ]
    }
   ],
   "source": [
    "# In this case it is even possible to name the column\n",
    "spark.createDataFrame(ages, 'ages int').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48c19bab-d148-4622-a421-c8b6bb59d6ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPySparkTypeError\u001B[0m                          Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-6135996068620151>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# But then giving only the data type or column name is erroneous:\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(ages, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mint\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1605\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n",
       "\u001B[1;32m   1600\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n",
       "\u001B[1;32m   1601\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n",
       "\u001B[1;32m   1602\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateDataFrame(  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n",
       "\u001B[1;32m   1603\u001B[0m         data, schema, samplingRatio, verifySchema\n",
       "\u001B[1;32m   1604\u001B[0m     )\n",
       "\u001B[0;32m-> 1605\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_dataframe(\n",
       "\u001B[1;32m   1606\u001B[0m     data, schema, samplingRatio, verifySchema  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n",
       "\u001B[1;32m   1607\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1662\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n",
       "\u001B[1;32m   1660\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromRDD(data\u001B[38;5;241m.\u001B[39mmap(prepare), schema, samplingRatio)\n",
       "\u001B[1;32m   1661\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1662\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromLocal(\u001B[38;5;28mmap\u001B[39m(prepare, data), schema)\n",
       "\u001B[1;32m   1663\u001B[0m jrdd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mSerDeUtil\u001B[38;5;241m.\u001B[39mtoJavaArray(rdd\u001B[38;5;241m.\u001B[39m_to_java_object_rdd())\n",
       "\u001B[1;32m   1664\u001B[0m jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsparkSession\u001B[38;5;241m.\u001B[39mapplySchemaToPythonRDD(jrdd\u001B[38;5;241m.\u001B[39mrdd(), struct\u001B[38;5;241m.\u001B[39mjson())\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1229\u001B[0m, in \u001B[0;36mSparkSession._createFromLocal\u001B[0;34m(self, data, schema)\u001B[0m\n",
       "\u001B[1;32m   1221\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_createFromLocal\u001B[39m(\n",
       "\u001B[1;32m   1222\u001B[0m     \u001B[38;5;28mself\u001B[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001B[38;5;28mstr\u001B[39m]]]\n",
       "\u001B[1;32m   1223\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRDD[Tuple]\u001B[39m\u001B[38;5;124m\"\u001B[39m, StructType]:\n",
       "\u001B[1;32m   1224\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   1225\u001B[0m \u001B[38;5;124;03m    Create an RDD for DataFrame from a list or pandas.DataFrame, returns the RDD and schema.\u001B[39;00m\n",
       "\u001B[1;32m   1226\u001B[0m \u001B[38;5;124;03m    This would be broken with table acl enabled as user process does not have permission to\u001B[39;00m\n",
       "\u001B[1;32m   1227\u001B[0m \u001B[38;5;124;03m    write temp files.\u001B[39;00m\n",
       "\u001B[1;32m   1228\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 1229\u001B[0m     internal_data, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wrap_data_schema(data, schema)\n",
       "\u001B[1;32m   1230\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39mparallelize(internal_data), struct\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1193\u001B[0m, in \u001B[0;36mSparkSession._wrap_data_schema\u001B[0;34m(self, data, schema)\u001B[0m\n",
       "\u001B[1;32m   1188\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_wrap_data_schema\u001B[39m(\n",
       "\u001B[1;32m   1189\u001B[0m     \u001B[38;5;28mself\u001B[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001B[38;5;28mstr\u001B[39m]]]\n",
       "\u001B[1;32m   1190\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Iterable[Tuple], StructType]:\n",
       "\u001B[1;32m   1191\u001B[0m     \u001B[38;5;66;03m# make sure data could consumed multiple times\u001B[39;00m\n",
       "\u001B[1;32m   1192\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, \u001B[38;5;28mlist\u001B[39m):\n",
       "\u001B[0;32m-> 1193\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(data)\n",
       "\u001B[1;32m   1195\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n",
       "\u001B[1;32m   1196\u001B[0m         struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inferSchemaFromList(data, names\u001B[38;5;241m=\u001B[39mschema)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1636\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe.<locals>.prepare\u001B[0;34m(obj)\u001B[0m\n",
       "\u001B[1;32m   1634\u001B[0m \u001B[38;5;129m@no_type_check\u001B[39m\n",
       "\u001B[1;32m   1635\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprepare\u001B[39m(obj):\n",
       "\u001B[0;32m-> 1636\u001B[0m     verify_func(obj)\n",
       "\u001B[1;32m   1637\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (obj,)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2913\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify\u001B[0;34m(obj)\u001B[0m\n",
       "\u001B[1;32m   2911\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mverify\u001B[39m(obj: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m   2912\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m verify_nullability(obj):\n",
       "\u001B[0;32m-> 2913\u001B[0m         verify_value(obj)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2773\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify_integer\u001B[0;34m(obj)\u001B[0m\n",
       "\u001B[1;32m   2771\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mverify_integer\u001B[39m(obj: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m   2772\u001B[0m     assert_acceptable_types(obj)\n",
       "\u001B[0;32m-> 2773\u001B[0m     verify_acceptable_types(obj)\n",
       "\u001B[1;32m   2774\u001B[0m     lower_bound \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2147483648\u001B[39m\n",
       "\u001B[1;32m   2775\u001B[0m     upper_bound \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2147483647\u001B[39m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2681\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify_acceptable_types\u001B[0;34m(obj)\u001B[0m\n",
       "\u001B[1;32m   2679\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(obj) \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m _acceptable_types[_type]:\n",
       "\u001B[1;32m   2680\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[0;32m-> 2681\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n",
       "\u001B[1;32m   2682\u001B[0m             error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFIELD_DATA_TYPE_UNACCEPTABLE_WITH_NAME\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   2683\u001B[0m             message_parameters\u001B[38;5;241m=\u001B[39m{\n",
       "\u001B[1;32m   2684\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfield_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mstr\u001B[39m(name),\n",
       "\u001B[1;32m   2685\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mstr\u001B[39m(dataType),\n",
       "\u001B[1;32m   2686\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobj\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mrepr\u001B[39m(obj),\n",
       "\u001B[1;32m   2687\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobj_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mtype\u001B[39m(obj)),\n",
       "\u001B[1;32m   2688\u001B[0m             },\n",
       "\u001B[1;32m   2689\u001B[0m         )\n",
       "\u001B[1;32m   2690\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n",
       "\u001B[1;32m   2691\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFIELD_DATA_TYPE_UNACCEPTABLE\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   2692\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   2696\u001B[0m         },\n",
       "\u001B[1;32m   2697\u001B[0m     )\n",
       "\n",
       "\u001B[0;31mPySparkTypeError\u001B[0m: [FIELD_DATA_TYPE_UNACCEPTABLE_WITH_NAME] field value: IntegerType() can not accept object (13,) in type <class 'tuple'>."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "PySparkTypeError",
        "evalue": "[FIELD_DATA_TYPE_UNACCEPTABLE_WITH_NAME] field value: IntegerType() can not accept object (13,) in type <class 'tuple'>."
       },
       "metadata": {
        "errorSummary": "[FIELD_DATA_TYPE_UNACCEPTABLE_WITH_NAME] field value: IntegerType() can not accept object (13,) in type <class 'tuple'>."
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "FIELD_DATA_TYPE_UNACCEPTABLE_WITH_NAME",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "sqlState": null,
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPySparkTypeError\u001B[0m                          Traceback (most recent call last)",
        "File \u001B[0;32m<command-6135996068620151>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# But then giving only the data type or column name is erroneous:\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(ages, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mint\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1605\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m   1600\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n\u001B[1;32m   1601\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n\u001B[1;32m   1602\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateDataFrame(  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n\u001B[1;32m   1603\u001B[0m         data, schema, samplingRatio, verifySchema\n\u001B[1;32m   1604\u001B[0m     )\n\u001B[0;32m-> 1605\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_dataframe(\n\u001B[1;32m   1606\u001B[0m     data, schema, samplingRatio, verifySchema  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m   1607\u001B[0m )\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1662\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m   1660\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromRDD(data\u001B[38;5;241m.\u001B[39mmap(prepare), schema, samplingRatio)\n\u001B[1;32m   1661\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1662\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromLocal(\u001B[38;5;28mmap\u001B[39m(prepare, data), schema)\n\u001B[1;32m   1663\u001B[0m jrdd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mSerDeUtil\u001B[38;5;241m.\u001B[39mtoJavaArray(rdd\u001B[38;5;241m.\u001B[39m_to_java_object_rdd())\n\u001B[1;32m   1664\u001B[0m jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsparkSession\u001B[38;5;241m.\u001B[39mapplySchemaToPythonRDD(jrdd\u001B[38;5;241m.\u001B[39mrdd(), struct\u001B[38;5;241m.\u001B[39mjson())\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1229\u001B[0m, in \u001B[0;36mSparkSession._createFromLocal\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m   1221\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_createFromLocal\u001B[39m(\n\u001B[1;32m   1222\u001B[0m     \u001B[38;5;28mself\u001B[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001B[38;5;28mstr\u001B[39m]]]\n\u001B[1;32m   1223\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRDD[Tuple]\u001B[39m\u001B[38;5;124m\"\u001B[39m, StructType]:\n\u001B[1;32m   1224\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1225\u001B[0m \u001B[38;5;124;03m    Create an RDD for DataFrame from a list or pandas.DataFrame, returns the RDD and schema.\u001B[39;00m\n\u001B[1;32m   1226\u001B[0m \u001B[38;5;124;03m    This would be broken with table acl enabled as user process does not have permission to\u001B[39;00m\n\u001B[1;32m   1227\u001B[0m \u001B[38;5;124;03m    write temp files.\u001B[39;00m\n\u001B[1;32m   1228\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1229\u001B[0m     internal_data, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wrap_data_schema(data, schema)\n\u001B[1;32m   1230\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39mparallelize(internal_data), struct\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1193\u001B[0m, in \u001B[0;36mSparkSession._wrap_data_schema\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m   1188\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_wrap_data_schema\u001B[39m(\n\u001B[1;32m   1189\u001B[0m     \u001B[38;5;28mself\u001B[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001B[38;5;28mstr\u001B[39m]]]\n\u001B[1;32m   1190\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Iterable[Tuple], StructType]:\n\u001B[1;32m   1191\u001B[0m     \u001B[38;5;66;03m# make sure data could consumed multiple times\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, \u001B[38;5;28mlist\u001B[39m):\n\u001B[0;32m-> 1193\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(data)\n\u001B[1;32m   1195\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[1;32m   1196\u001B[0m         struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inferSchemaFromList(data, names\u001B[38;5;241m=\u001B[39mschema)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1636\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe.<locals>.prepare\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m   1634\u001B[0m \u001B[38;5;129m@no_type_check\u001B[39m\n\u001B[1;32m   1635\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprepare\u001B[39m(obj):\n\u001B[0;32m-> 1636\u001B[0m     verify_func(obj)\n\u001B[1;32m   1637\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (obj,)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2913\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m   2911\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mverify\u001B[39m(obj: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2912\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m verify_nullability(obj):\n\u001B[0;32m-> 2913\u001B[0m         verify_value(obj)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2773\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify_integer\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m   2771\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mverify_integer\u001B[39m(obj: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2772\u001B[0m     assert_acceptable_types(obj)\n\u001B[0;32m-> 2773\u001B[0m     verify_acceptable_types(obj)\n\u001B[1;32m   2774\u001B[0m     lower_bound \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2147483648\u001B[39m\n\u001B[1;32m   2775\u001B[0m     upper_bound \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2147483647\u001B[39m\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2681\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify_acceptable_types\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m   2679\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(obj) \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m _acceptable_types[_type]:\n\u001B[1;32m   2680\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 2681\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m   2682\u001B[0m             error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFIELD_DATA_TYPE_UNACCEPTABLE_WITH_NAME\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   2683\u001B[0m             message_parameters\u001B[38;5;241m=\u001B[39m{\n\u001B[1;32m   2684\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfield_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mstr\u001B[39m(name),\n\u001B[1;32m   2685\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mstr\u001B[39m(dataType),\n\u001B[1;32m   2686\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobj\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mrepr\u001B[39m(obj),\n\u001B[1;32m   2687\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobj_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mtype\u001B[39m(obj)),\n\u001B[1;32m   2688\u001B[0m             },\n\u001B[1;32m   2689\u001B[0m         )\n\u001B[1;32m   2690\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m   2691\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFIELD_DATA_TYPE_UNACCEPTABLE\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   2692\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2696\u001B[0m         },\n\u001B[1;32m   2697\u001B[0m     )\n",
        "\u001B[0;31mPySparkTypeError\u001B[0m: [FIELD_DATA_TYPE_UNACCEPTABLE_WITH_NAME] field value: IntegerType() can not accept object (13,) in type <class 'tuple'>."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# But then giving only the data type or column name is erroneous:\n",
    "spark.createDataFrame(ages, 'int').show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01 - Create Single Column Spark Dataframe using List",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}