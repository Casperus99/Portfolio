{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a9c4a9e-f268-4d0e-9bd4-a14a60350651",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SPARK ROWS - OVERVIEW\n",
    "list_of_lists = [[1, \"Kacper\"], [2, \"Michael\"], [3, \"Sophia\"]]\n",
    "df = spark.createDataFrame(list_of_lists, 'id int, name string')\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df.collect()  # Returns the dataframe as a list\n",
    "\n",
    "Row(1, \"Kacper\")\n",
    "row = Row(id=1, name=\"Kacper\")\n",
    "row.name\n",
    "row[\"name\"]\n",
    "\n",
    "list_of_rows = [Row(*list_obj) for list_obj in list_of_lists]\n",
    "spark.createDataFrame(list_of_rows, 'id int, name string').show()\n",
    "# It is possible to convert all those containers into Row objects but spark doesn't really need it to create a DataFrame correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a36de210-fcd4-4ab1-b68b-cdcddb14880e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "list_of_lists = [[1, \"Kacper\"], [2, \"Michael\"], [3, \"Sophia\"]]\n",
    "df = spark.createDataFrame(list_of_lists, 'id int, name string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a165ee55-0e68-45dd-91cd-83ab362bb03d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# .collect() - returns the dataFrame in the form of a built-in list\n",
    "# However it uses Row objects which come from pyspark.sql library\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "526591c8-f588-4cca-b90f-9f03d4eefc01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b926c0e2-8c4e-4c6e-ba54-ac1bd09cb318",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Row(1, \"Kacper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "252e75f8-7aa7-4bd3-b91b-ed9b5daa904e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Row(id=1, name=\"Kacper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a739489-0b13-4415-82cf-03c929fe4961",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "row = Row(id=1, name=\"Kacper\")\n",
    "# We can access the values of the Row object using the dot notation\n",
    "print(row.name)\n",
    "# or using the keys\n",
    "print(row[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c5ded1e-b353-4c2c-91e3-6b42c6292a6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We can use list of Row objects to create a DataFrame\n",
    "list_of_rows = [Row(*list_obj) for list_obj in list_of_lists]\n",
    "spark.createDataFrame(list_of_rows, 'id int, name string').show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03 - Overview of Spark Row",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
