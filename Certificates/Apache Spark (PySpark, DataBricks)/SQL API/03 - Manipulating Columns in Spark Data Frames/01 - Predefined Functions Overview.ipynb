{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bc2e427-aa7f-422a-9d9e-6374054287b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* ### String Manipulation Functions\n",
    "  * Case Conversion - `lower`,  `upper`\n",
    "  * Getting Length -  `length`\n",
    "  * Extracting substrings - `substring`, `split`\n",
    "  * Trimming - `trim`, `ltrim`, `rtrim`\n",
    "  * Padding - `lpad`, `rpad`\n",
    "  * Concatenating string - `concat`, `concat_ws`\n",
    "* ### Date Manipulation Functions\n",
    "  * Getting current date and time - `current_date`, `current_timestamp`\n",
    "  * Date Arithmetic - `date_add`, `date_sub`, `datediff`, `months_between`, `add_months`, `next_day`\n",
    "  * Beginning and Ending Date or Time - `last_day`, `trunc`, `date_trunc`\n",
    "  * Formatting Date - `date_format`\n",
    "  * Extracting Information - `dayofyear`, `dayofmonth`, `dayofweek`, `year`, `month`\n",
    "* ### Aggregate Functions\n",
    "  * `count`, `countDistinct`\n",
    "  * `sum`, `avg`\n",
    "  * `min`, `max`\n",
    "* ### Other Functions\n",
    "  * `CASE` and `WHEN`\n",
    "  * `CAST` for type casting\n",
    "  * Functions to manage special types such as `ARRAY`, `MAP`, `STRUCT` type columns\n",
    "  * Many others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac47a368-6ba3-44db-9966-de4992192862",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading data\n",
    "\n",
    "orders = spark.read.csv(\n",
    "    '/public/retail_db/orders',\n",
    "    schema='order_id INT, order_date STRING, order_customer_id INT, order_status STRING'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "817b54b2-933d-4fbf-b7a3-d4fe0ed99228",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We can find those functions in the pyspark.sql.functions:\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "806952d2-db29-4a42-bf1c-6974b7356981",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function concat_ws in module pyspark.sql.functions.builtin:\n\nconcat_ws(sep: str, *cols: 'ColumnOrName') -> pyspark.sql.column.Column\n    Concatenates multiple input string columns together into a single string column,\n    using the given separator.\n    \n    .. versionadded:: 1.5.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    sep : str\n        words separator.\n    cols : :class:`~pyspark.sql.Column` or str\n        list of columns to work on.\n    \n    Returns\n    -------\n    :class:`~pyspark.sql.Column`\n        string of concatenated words.\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n    >>> df.select(concat_ws('-', df.s, df.d).alias('s')).collect()\n    [Row(s='abcd-123')]\n\n"
     ]
    }
   ],
   "source": [
    "# We can use help() function to get the documentation about the function\n",
    "help(concat_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40e509ff-12b7-4a34-9229-f628f33f9bf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n|concat(order_id,  abc , order_date)|\n+-----------------------------------+\n|1 abc 2013-07-25 00:00:00.0        |\n|2 abc 2013-07-25 00:00:00.0        |\n|3 abc 2013-07-25 00:00:00.0        |\n|4 abc 2013-07-25 00:00:00.0        |\n|5 abc 2013-07-25 00:00:00.0        |\n|6 abc 2013-07-25 00:00:00.0        |\n|7 abc 2013-07-25 00:00:00.0        |\n|8 abc 2013-07-25 00:00:00.0        |\n|9 abc 2013-07-25 00:00:00.0        |\n|10 abc 2013-07-25 00:00:00.0       |\n|11 abc 2013-07-25 00:00:00.0       |\n|12 abc 2013-07-25 00:00:00.0       |\n|13 abc 2013-07-25 00:00:00.0       |\n|14 abc 2013-07-25 00:00:00.0       |\n|15 abc 2013-07-25 00:00:00.0       |\n|16 abc 2013-07-25 00:00:00.0       |\n|17 abc 2013-07-25 00:00:00.0       |\n|18 abc 2013-07-25 00:00:00.0       |\n|19 abc 2013-07-25 00:00:00.0       |\n|20 abc 2013-07-25 00:00:00.0       |\n+-----------------------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# It's important to remember that some of these functions doesn't work with column names (strings)\n",
    "# They need col() or lit() object as an argument.\n",
    "\n",
    "# For example concat() needs lit():\n",
    "orders.select(concat('order_id', lit(' abc '), 'order_date')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd79cce7-32cc-4c31-a1d4-8a3e38d140ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n|order_id_alias|\n+--------------+\n|             1|\n|             2|\n|             3|\n|             4|\n|             5|\n|             6|\n|             7|\n|             8|\n|             9|\n|            10|\n|            11|\n|            12|\n|            13|\n|            14|\n|            15|\n|            16|\n|            17|\n|            18|\n|            19|\n|            20|\n+--------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# And .alias() works only on col() object:\n",
    "orders.select(col('order_id').alias('order_id_alias')).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01 - Predefined Functions Overview",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}