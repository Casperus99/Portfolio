{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c8d2605-d89a-4c80-890e-1ef7924cf77c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orders = spark.read.csv(\n",
    "    '/public/retail_db/orders',\n",
    "    schema='order_id INT, order_date STRING, order_customer_id INT, order_status STRING'\n",
    ")\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6a54b68-ad4a-453a-819d-f7a4f8a0eee4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DROPPING COLUMNS\n",
    "\n",
    "orders.drop(\"order_status\")\n",
    "\n",
    "cols_to_drop = [col('order_id'), col('order_date')]\n",
    "orders.drop(*cols_to_drop)\n",
    "\n",
    "\n",
    "# DROPPING ROWS\n",
    "\n",
    "orders.distinct()\n",
    "orders.dropDuplicates(['order_date', 'order_customer_id'])\n",
    "\n",
    "orders.na.drop(how='any', thresh=3, subset=['order_date', 'order_customer_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e38553bc-05c0-425d-86dd-633d5dceaeea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------+-----------------+\n|order_id|order_date           |order_customer_id|\n+--------+---------------------+-----------------+\n|1       |2013-07-25 00:00:00.0|11599            |\n|2       |2013-07-25 00:00:00.0|256              |\n|3       |2013-07-25 00:00:00.0|12111            |\n+--------+---------------------+-----------------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Typical column dropping:\n",
    "\n",
    "orders.drop(\"order_status\").show(n=3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45afeb24-9b73-4036-913c-e9b2b0329e38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------+\n|order_customer_id|order_status   |\n+-----------------+---------------+\n|11599            |CLOSED         |\n|256              |PENDING_PAYMENT|\n|12111            |COMPLETE       |\n+-----------------+---------------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "# We can also drop multiple columns at once\n",
    "\n",
    "cols_to_drop = [col('order_id'), col('order_date')]\n",
    "orders.drop(*cols_to_drop).show(n=3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8a3035d-f3e1-4c5e-97a9-d5f7a4999bdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------+-----------------+---------------+\n|order_id|order_date           |order_customer_id|order_status   |\n+--------+---------------------+-----------------+---------------+\n|1       |2013-07-25 00:00:00.0|11599            |CLOSED         |\n|2       |2013-07-25 00:00:00.0|256              |PENDING_PAYMENT|\n|3       |2013-07-25 00:00:00.0|12111            |COMPLETE       |\n+--------+---------------------+-----------------+---------------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Dropping column that doesn't exist is ignored:\n",
    "\n",
    "orders.drop(\"order_sample\").show(n=3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75c25be9-3bed-4cfa-b56d-aa6a76a83e3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68883\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "364"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.distinct() - distinct is applied to all columns\n",
    "\n",
    "print(orders.count())\n",
    "orders.select('order_date').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5efae4a3-fba5-4883-b171-dc4da19aff3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68883\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "68321"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.dropDuplicates(subset<list>) - lets us specify which columns to consider for duplicate detection\n",
    "\n",
    "print(orders.count())\n",
    "orders.dropDuplicates(['order_date', 'order_customer_id']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a0628bb-d349-4abe-b999-04113ba18385",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method drop in module pyspark.sql.dataframe:\n\ndrop(how: str = 'any', thresh: Optional[int] = None, subset: Union[str, Tuple[str, ...], List[str], NoneType] = None) -> pyspark.sql.dataframe.DataFrame method of pyspark.sql.dataframe.DataFrameNaFunctions instance\n    Returns a new :class:`DataFrame` omitting rows with null values.\n    :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are\n    aliases of each other.\n    \n    .. versionadded:: 1.3.1\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    how : str, optional, the values that can be 'any' or 'all', default 'any'.\n        If 'any', drop a row if it contains any nulls.\n        If 'all', drop a row only if all its values are null.\n    thresh: int, optional, default None.\n        If specified, drop rows that have less than `thresh` non-null values.\n        This overwrites the `how` parameter.\n    subset : str, tuple or list, optional\n        optional list of column names to consider.\n    \n    Returns\n    -------\n    :class:`DataFrame`\n        DataFrame with null only rows excluded.\n    \n    Examples\n    --------\n    >>> from pyspark.sql import Row\n    >>> df = spark.createDataFrame([\n    ...     Row(age=10, height=80, name=\"Alice\"),\n    ...     Row(age=5, height=None, name=\"Bob\"),\n    ...     Row(age=None, height=None, name=\"Tom\"),\n    ...     Row(age=None, height=None, name=None),\n    ... ])\n    \n    Example 1: Drop the row if it contains any nulls.\n    \n    >>> df.na.drop().show()\n    +---+------+-----+\n    |age|height| name|\n    +---+------+-----+\n    | 10|    80|Alice|\n    +---+------+-----+\n    \n    Example 2: Drop the row only if all its values are null.\n    \n    >>> df.na.drop(how='all').show()\n    +----+------+-----+\n    | age|height| name|\n    +----+------+-----+\n    |  10|    80|Alice|\n    |   5|  NULL|  Bob|\n    |NULL|  NULL|  Tom|\n    +----+------+-----+\n    \n    Example 3: Drop rows that have less than `thresh` non-null values.\n    \n    >>> df.na.drop(thresh=2).show()\n    +---+------+-----+\n    |age|height| name|\n    +---+------+-----+\n    | 10|    80|Alice|\n    |  5|  NULL|  Bob|\n    +---+------+-----+\n    \n    Example 4: Drop rows with non-null values in the specified columns.\n    \n    >>> df.na.drop(subset=['age', 'name']).show()\n    +---+------+-----+\n    |age|height| name|\n    +---+------+-----+\n    | 10|    80|Alice|\n    |  5|  NULL|  Bob|\n    +---+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# To drop nulls we can use 2 options:\n",
    "#   df.na.drop()\n",
    "#   df.dropna()\n",
    "\n",
    "'''\n",
    "    Parameters\n",
    "    ----------\n",
    "    how : str, optional, the values that can be 'any' or 'all', default 'any'.\n",
    "        If 'any', drop a row if it contains any nulls.\n",
    "        If 'all', drop a row only if all its values are null.\n",
    "    thresh: int, optional, default None.\n",
    "        If specified, drop rows that have less than `thresh` non-null values.\n",
    "        This overwrites the `how` parameter.\n",
    "    subset : str, tuple or list, optional\n",
    "        optional list of column names to consider.\n",
    "'''\n",
    "\n",
    "help(orders.na.drop)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06 - Dropping columns and rows",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}